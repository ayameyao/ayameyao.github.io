---
title: "Ayao's Homepage"
layout: default
---

<!-- <div style="position: fixed; bottom: 15px; right:1px;">
  <a href=""> <img src="{{ site.baseurl }}/static/img/logo/cn.png" width="50%"; /> </a>
</div> -->


<!-- About -->
<section id="about">
  <div class="container">
    <br>
    <br>

    <!-- personal image -->
    <div id="profile" class="profile-img col-md-4">
      
      <div class="team-member">
        <a href="https://ayameyao.github.io/">
          <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/guangyaoli.jpg" />
        </a>
        <div class="porfile-name"><h3>Guangyao Li</h3></div>
        <div class="profile-student text-muted"><h5>PhD Candidate</h5></div>
        <h6><a href="https://www.ruc.edu.cn/">Renmin University of China</a></h6>
      </div>  

      <ul class="network-icon" aria-hidden="true">
        <li>
          <a itemprop="sameAs" href="mailto:guangyaoli@ruc.edu.au" target="_blank" rel="noopener">
            <i class="fa fa-envelope big-icon"></i>
          </a>
        </li>
        <li>
          <a itemprop="sameAs" href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=i_KZwgQAAAAJ" target="_blank" rel="noopener">
            <i class="ai ai-google-scholar big-icon"></i>
          </a>
        </li>  
        <li>
          <a itemprop="sameAs" href="https://github.com/ayameyao" target="_blank" rel="noopener">
            <i class="fa fa-github big-icon"></i>
          </a>
        </li>        
      </ul>

      <br>
      <br>
      <br>
      

    </div>

    


    <div class="profile-img col-md-8" style="float:right">
      <h1>Biography</h1>
      
      <p class="text-muted">
        I am a postdoctoral researcher at Tsinghua University, working with 
        Prof. <a href="https://scholar.google.com/citations?user=7t2jzpgAAAAJ&hl=en">Wenwu Zhu </a>
        and 
        Prof. <a href = "https://mn.cs.tsinghua.edu.cn/xinwang/">Xin Wang</a>.
        I received my Ph.D. from Renmin University of China, supervised by 
        Prof. <a href="https://gewu-lab.github.io/">Di Hu</a> 
        at the Gaoling School of Artificial Intelligence.
        My recently research interests include audio-visual learning, scene understanding and Embodied AI.
        <!-- Here are my <a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=i_KZwgQAAAAJ">Google Scholar</a>. -->
      </p>

      <div class="row">
        <div class="col-lg-12 text-left">
          <h4 class="section-subheading" style="text-align:left;">News</h4>
          <ul class="text-muted">
            <li>[07-2024] One paper accepted by ACMMM, thanks to all co-authors!</li>
            <li>[05-2024] One paper accepted by TOMM, thanks to all co-authors!</li>
            <li>[05-2024] One paper accepted by CVPR Workshop, thanks to all co-authors!</li>
            <li>[07-2023] One paper accepted by ACMMM, thanks to all co-authors!</li>
            <li>[05-2023] One paper accepted by INTERSPEECH (<font color="#dc3545"><b>Oral</b></font>), thanks to all co-authors!</li>
            <!-- <li>[11-2022] One paper accepted by IJAEOG, thanks to all co-authors!</li> -->
            <li>[03-2022] One paper accepted by CVPR (<font color="#dc3545"><b>Oral</b></font>), thanks to all co-authors!</li>
            <!-- <li>[03-2021] Predicting sound by clicking on Google Earth! One collaborative work with TUM, DLR and MIT. Here are the <a href="https://arxiv.org/abs/2108.00688">paper</a> and <a href="https://www.youtube.com/watch?v=gD_fNJPBWhs">demo</a>!</li> -->
            <li>[08-2020] I will join <a href="https://dtaoo.github.io/group.html">GeWu-Lab</a> to pursue a PhD degree at <a href="https://www.ruc.edu.cn/">Renmin University of China</a>!</li>
          </ul>
        </div>
      </div>

    </div>
      
  </div>
</section>


<br>
<br>
<br>
<br>
<br>
<br>
<br>




<!-- Publications -->
<section class="bg-light" id="publications" style="clear:both;">
  <div class="container">
    <!-- <div class="row justify-content-md-center text-center"> -->
    <div class="row text-center">
      <div class="col-md-12" style="text-align:left">
        <h1 class="section-subheading" style="text-align:left; margin-left:-14px">Selected Publications</h1>
      </div>

      <hr/>

      <p style="text-align:left"> &nbsp;&nbsp;<sup>†</sup> indicates equal contribution.  </p>
      <p style="text-align:left"> &nbsp;&nbsp;Most recent publications on 
        <a href="https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&user=i_KZwgQAAAAJ">Google Scholar</a>.  </p>

      <!-- publication 12 -->
      <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/2024_ACMMM2.png" />
        </div>
        <div class="publication-right" >
          <div class="text-muted publication-content">
            <h5 class="pub-title">Boosting Audio Visual Question Answering via Key Semantic-Aware Cues</h5>
            <div class="pub-authors">
              <b>Guangyao Li</b>, Henghui Du, Di Hu
            </div>
            <div class="pub-publication">
              <i>Proc. ACM International Conference on Multimedia (ACM MM)</i>, 2024.
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <a href="">[Paper]</a>&nbsp;
              <a href="">[arXiv]</a>&nbsp;
              <a href="">[Code]</a>
            </p>
          </div>
        </div>
      </div>

      <br>



      <!-- publication 11 -->
      <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/2024_CVPRW.png" />
        </div>
        <div class="publication-right" >
          <div class="text-muted publication-content">
            <h5 class="pub-title">AVQA-CoT: When CoT Meets Question Answering in Audio-Visual Scenarios </h5>
            <div class="pub-authors">
              <b>Guangyao Li</b>, Henghui Du, Di Hu
            </div>
            <div class="pub-publication">
              <i>CVPR Sight and Sound Workshops</i>, 2024.
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <a href="https://sightsound.org/papers/2024/Li_AVQA-CoT_When_CoT_Meets_Question_Answering_in_Audio-Visual_Scenarios.pdf">[Paper]</a>&nbsp;
              <a href="">[arXiv]</a>&nbsp;
              <a href="">[Code]</a>
            </p>
          </div>
        </div>
      </div>

      <br>


      <!-- publication 10 -->
      <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/2023_lfav.png" />
        </div>
        <div class="publication-right" >
          <div class="text-muted publication-content">
            <h5 class="pub-title">Towards Long Form Audio-visual Video Understanding </h5>
            <div class="pub-authors">
              Wenxuan Hou<sup>†</sup>, <b>Guangyao Li<sup>†</sup></b>, Yapeng Tian, Di Hu
            </div>
            <div class="pub-publication">
              <i>ACM Transactions on Multimedia Computing, Communications and Applications (TOMM)</i>, 2024.
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <a href="https://gewu-lab.github.io/LFAV/">[Project]</a>&nbsp;
              <!-- <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.pdf">[Paper]</a>&nbsp; -->
              <!-- </a><a href="{{ site.baseurl }}/static/files/MUSIC-AVQA-supp.pdf">[Supp]</a>&nbsp; -->
              <!-- </a><a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Li_Learning_To_Answer_CVPR_2022_supplemental.pdf">[Supp]</a>&nbsp; -->
              <a href="https://arxiv.org/abs/2306.09431">[arXiv]</a>&nbsp;
              <!-- <a href="{{ site.baseurl }}/static/files/MUSIC-AVQA-poster.pdf">[Poster]</a>&nbsp; -->
              <!-- <a href="https://www.youtube.com/watch?v=JH3t5gwe9Xw">[YouTube]</a>&nbsp; -->
              <!-- <a href="https://www.bilibili.com/video/BV1Br4y1q7YN/">[Bilibili]</a>&nbsp; -->
              <a href="https://github.com/GeWu-Lab/LFAV">[Code]</a>
            </p>
          </div>
        </div>
      </div>

      <br>

      <!-- publication 09 -->
      <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/2023_iccv.png" />
        </div>
        <div class="publication-right" >
          <div class="text-muted publication-content">
            <h5 class="pub-title">Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer</h5>
            <div class="pub-authors">
              Yaoting Wang, Weisong Liu, <b>Guangyao Li</b>, Jian Ding, Di Hu, Xi Li
            </div>
            <div class="pub-publication">
              <!-- <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2022. -->
              <i>The 38th Annual AAAI Conference on Artificial Intelligence (AAAI)</i>, 2024.
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28378">[Paper]</a>&nbsp;
              <a href="https://arxiv.org/abs/2309.07929">[arXiv]</a>&nbsp;
              <a href="https://github.com/GeWu-Lab/Generalizable-Audio-Visual-Segmentation">[Code]</a>&nbsp;
              <a href="https://underline.io/lecture/93666-prompting-segmentation-with-sound-is-generalizable-audio-visual-source-localizer">[Video]</a>
            </p>
          </div>
        </div>
      </div>

      <br>

      <!-- publication 08 -->
      <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/2023_acmmm_avqa2.png" />
        </div>
        <div class="publication-right" >
          <div class="text-muted publication-content">
            <h5 class="pub-title">Progressive Spatio-temporal Perception for Audio-Visual Question Answering</h5>
            <div class="pub-authors">
              <b>Guangyao Li</b>, Wenxuan Hou, Di Hu
            </div>
            <div class="pub-publication">
              <!-- <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2022. -->
              <i>Proc. ACM International Conference on Multimedia (ACM MM)</i>, 2023.
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <a href="https://arxiv.org/abs/2308.05421">[Paper]</a>&nbsp;
              <a href="https://arxiv.org/abs/2308.05421">[arXiv]</a>&nbsp;
              <a href="https://github.com/GeWu-Lab/PSTP-Net">[Code]</a>
            </p>
          </div>
        </div>
      </div>

      <br>

      


      <!-- publication 07 -->
      <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/2020_Interspeech_aqa1.png" />
        </div>
        <div class="publication-right" >
          <div class="text-muted publication-content">
            <h5 class="pub-title">Multi-Scale Attention for Audio Question Answering (<font color="#dc3545"><b>Oral</b></font>)</h5>
            <div class="pub-authors">
              <b>Guangyao Li</b>, Yixin Xu, Di Hu
            </div>
            <div class="pub-publication">
              <!-- <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2022. -->
              <i>Proc. Conference of the International Speech Communication Association (INTERSPEECH)</i>, 2023.
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <a href="https://arxiv.org/abs/2305.17993">[Paper]</a>&nbsp;
              <a href="https://arxiv.org/abs/2305.17993">[arXiv]</a>&nbsp;
              <a href="https://github.com/GeWu-Lab/MWAFM">[Code]</a>
            </p>
          </div>
        </div>
      </div>

      <br>



      <!-- publication 06 -->
      <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/music-avqa.png" />
        </div>
        <div class="publication-right" >
          <div class="text-muted publication-content">
            <h5 class="pub-title">Learning to Answer Questions in Dynamic Audio-Visual Scenarios (<font color="#dc3545"><b>Oral</b></font>)</h5>
            <div class="pub-authors">
              <b>Guangyao Li<sup>†</sup></b>, Yake Wei<sup>†</sup>, Yapeng Tian<sup>†</sup>, Chenliang Xu, Ji-Rong Wen and Di Hu
            </div>
            <div class="pub-publication">
              <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2022.
              <!-- <i>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2022. -->
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <a href="https://gewu-lab.github.io/MUSIC-AVQA/">[Project]</a>&nbsp;
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Learning_To_Answer_Questions_in_Dynamic_Audio-Visual_Scenarios_CVPR_2022_paper.pdf">[Paper]</a>&nbsp;
              <!-- </a><a href="{{ site.baseurl }}/static/files/MUSIC-AVQA-supp.pdf">[Supp]</a>&nbsp; -->
              </a><a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Li_Learning_To_Answer_CVPR_2022_supplemental.pdf">[Supp]</a>&nbsp;
              <a href="https://arxiv.org/abs/2203.14072">[arXiv]</a>&nbsp;
              <a href="{{ site.baseurl }}/static/files/MUSIC-AVQA-poster.pdf">[Poster]</a>&nbsp;
              <a href="https://www.youtube.com/watch?v=JH3t5gwe9Xw">[YouTube]</a>&nbsp;
              <a href="https://www.bilibili.com/video/BV1Br4y1q7YN/">[Bilibili]</a>&nbsp;
              <a href="https://github.com/GeWu-Lab/MUSIC-AVQA">[Code]</a>
            </p>
          </div>
        </div>
      </div>

      <br>

      <!-- publication 05 -->
      <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/LooktoHearOurPlanet.png" />
        </div>
        <div class="publication-right">
          <div class="text-muted publication-content">
            <h5 class="pub-title">Self-supervised Audiovisual Representation Learning for Remote Sensing Data</h5>
            <div class="pub-authors">
              Konrad Heidler, Lichao Mou, Di Hu, Pu Jin, <b>Guangyao Li</b>, Chuang Gan, Ji-Rong Wen, Xiao Xiang Zhu
            </div>
            <div class="pub-publication">
              <i>International Journal of Applied Earth Observation and Geoinformation (IJAEOG)</i>, 2023.
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <a href="https://www.sciencedirect.com/science/article/pii/S1569843222003181">[Paper]</a>&nbsp;
              <a href="https://arxiv.org/abs/2108.00688">[arXiv]</a>&nbsp;
              <a href="https://www.youtube.com/watch?v=gD_fNJPBWhs">[Demo](YouTube)</a>
            </p>
          </div>
        </div>
      </div>

      <br>


      <p class="text-muted" style="text-align:left">
        <br>
        Before 2020, my research interests mainly focus on agricultural artificial intelligence and agricultural informatization.
      </p>
      
      <!-- publication 04 -->
      <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/2020_CompAgri_review.png" />
        </div>
        <div class="publication-right">
          <div class="text-muted publication-content">
            <h5 class="pub-title">A review of computer vision technologies for plant phenotyping</h5>
            <div class="pub-authors">
              Zhenbo Li, Ruohao Guo, Meng Li, Yaru Chen, <b>Guangyao Li</b>
            </div>
            <div class="pub-publication">
              <i>Computers and Electronics in Agriculture (COMPAG)</i>, 2020.
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <!-- <a href="{{ site.baseurl }}/static/files/publications/A_review_of_computer_vision_technologies_for_plant_phenotyping.pdf">[Pdf]&nbsp;</a> -->
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0168169920307511">[Paper]</a>
              <!-- <a href="hhttps://www.youtube.com/watch?v=gD_fNJPBWhs" style="color:#ffc107">[Demo]</a>&nbsp; -->
            </p>
          </div>
        </div>
      </div>

      <br>

      <!-- publication 03 -->
      <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/2019_prcv_shellfish.png" />
        </div>
        <div class="publication-right">
          <div class="text-muted publication-content">
            <h5 class="pub-title">Shellfish Detection based on Fusion Attention Mechanism in End-to-End Network</h5>
            <div class="pub-authors">
              <b>Guangyao Li</b>, Zhenbo Li, Chuyue Zhang, Yaodong Li, Jun Yue
            </div>
            <div class="pub-publication">
              <!-- <i>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2022. -->
              <i>Proc. Conference on Pattern Recognition and Computer Vision (PRCV)</i>. 2019.
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <a href="https://dl.acm.org/doi/abs/10.1007/978-3-030-31726-3_44">[Paper]</a>
              <!-- <a href="https://link.springer.com/chapter/10.1007/978-3-030-31726-3_44">[Springer]</a> -->
              <!-- <a href="hhttps://www.youtube.com/watch?v=gD_fNJPBWhs" style="color:#ffc107">[Demo]</a>&nbsp; -->
            </p>
          </div>
        </div>
      </div>

      <br>


      <!-- publication 02 -->
      <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/2018_ifac_cucumber.png" />
        </div>
        <div class="publication-right">
          <div class="text-muted publication-content">
            <h5 class="pub-title">Sea cucumber image dehazing method by fusion of Retinex and dark channel</h5>
            <div class="pub-authors">
              Zhenbo Li, <b>Guangyao Li</b>, Bingshan Niu, Fang Peng
            </div>
            <div class="pub-publication">
              <i>IFAC PapersOnLine</i>, 2018.
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <a href="https://www.sciencedirect.com/science/article/pii/S240589631831214X?via%3Dihub">[Paper]</a>
              <!-- <a href="hhttps://www.youtube.com/watch?v=gD_fNJPBWhs" style="color:#ffc107">[Demo]</a>&nbsp; -->
            </p>
          </div>
        </div>
      </div>

      <br>

      <!-- publication 01 -->
      <!-- <div class="publication">
        <div class="publication-left">
          <img src="{{ site.baseurl }}/static/files/publications/2018_ifac_waterquality.png" />
        </div>
        <div class="publication-right">
          <div class="text-muted publication-content">
            <h5 class="pub-title">Water Quality Prediction Model Combining Sparse Auto-encoder and LSTM Network</h5>
            <div class="pub-authors">
              Zhenbo Li, Fang Peng, Bingshan Niu, <b>Guangyao Li</b>, Jing Wu, Zheng Miao
            </div>
            <div class="pub-publication">
              <i>IFAC PapersOnLine</i>, 2018.
            </div>
            <p class="text-muted" style="color:#00F; margin-bottom:0rem; line-height: 1.5">
              <a href="https://www.sciencedirect.com/science/article/pii/S2405896318312072">[Paper]</a>
            </p>
          </div>
        </div>
      </div> -->


    </div>
    </div>
  </div>
</section>












<!-- Service -->
<section id="service" style="clear:both;">
  <div class="container">
    <!-- <div class="row justify-content-md-center text-center"> -->
    <!-- <div class="row text-center"> -->
    <div class="row">
      <div class="col-md-12" style="text-align:left">
        <h1 class="section-subheading" style="text-align:left; margin-left:-14px">Service</h1>
        <h1 class="section-subheading" style="text-align:left; margin-left:-14px"><hr/></h1>
      </div>

      

      <br>
      <br>

      <!-- <ul> -->
        <!-- <b>PC Member:</b> IJCAI 2023, AAAI 2024 -->
<!--         <ul>
          <li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2023, 2024,</li>
          <li>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2023, 2024,</li>
          <li>International Joint Conference on Artificial Intelligence (IJCAI) 2023.</li>
          <li>Asian Conference on Computer Vision (ACCV) 2022,</li>
        </ul> -->
      <!-- </ul> -->

      <ul>
        <b>Conference Reviewer:</b>
        <ul>
          <li>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2023, 2024,</li>
          <li>ACM International Conference on Multimedia (ACMMM) 2024,</li>
          <li>Annual Conference on Neural Information Processing Systems (NeurIPS) 2024,</li>
          <li>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2023, 2024,</li>
          <li>International Joint Conference on Artificial Intelligence (IJCAI) 2023, 2024,</li>
          <li>Asian Conference on Computer Vision (ACCV) 2022.</li>
        </ul>
      </ul>

      <ul>
        <b>Journal Reviewer:</b>
        <ul>
          <li>IEEE Transactions on Multimedia (TMM),</li>
          <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT).</li>
        </ul>
      </ul>


    </div>
  </div>
</section>









<!-- Contact -->
<section  id="contact">
<!-- <section class="bg-light" id="contact"> -->
  <div class="container">
    <br>
    <h2 class="section-heading text-uppercase">Contact</h2>
    <div class="text-muted">
        Email: guangyaoli@ruc.edu.cn
        <br>
        Address: Lide Building
    </div>
  </div>
</section>



<script type="application/ld+json">
{
  "@context":"http://schema.org/",
  "@type":"Dataset",
  "name":"MUSIC-AVQA dataset",
  "description":"First-person (egocentric) video dataset; multi-faceted non-scripted recordings in the wearers' homes, capturing all daily activities in the kitchen over multiple days. Annotations are collected using a novel live audio commentary approach.",
  "url":"https://github.com/epic-kitchens/annotations",
  "sameAs":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d",
  "citation":"Damen, Dima et al. 'Scaling Egocentric Vision: The EPIC-KITCHENS Dataset', European Conference on Computer Vision, 2018",
  "identifier": "10.5523/bris.3h91syskeag572hl6tvuovwv4d",
  "keywords":[
     "Egocentric vision",
     "Human actions",
     "Object interactions",
     "actions",
     "video",
     "kitchens",
     "cooking",
     "dataset",
     "epic kitchens",
     "epic",
     "eccv",
     "2022"
  ],
  "creator":{
     "@type":"Organization",
     "url": "https://epic-kitchens.github.io/",
     "name":"EPIC Team",
     "contactPoint":{
        "@type":"ContactPoint",
        "contactType": "technical support",
        "email":"uob-epic-kitchens@bristol.ac.uk",
        "url":"https://github.com/epic-kitchens/annotations/issues"
     }
  },
  "distribution":[
     {
        "@type":"DataDownload",
        "encodingFormat":"video/mp4",
        "contentUrl":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"image/jpeg",
        "contentUrl":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"text/csv",
        "contentUrl":"https://github.com/epic-kitchens/annotations"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"application/octet-stream",
        "contentUrl":"https://github.com/epic-kitchens/annotations"
     }
  ],
  "license": "https://creativecommons.org/licenses/by-nc/4.0/"
}
</script>

