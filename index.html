---
title: "Ayao's Homepage"
layout: default
---

<!-- <div style="position: fixed; bottom: 15px; right:1px;">
  <a href=""> <img src="{{ site.baseurl }}/static/img/logo/cn.png" width="50%"; /> </a>
</div> -->


<!-- About -->
<section id="about">
  <div class="container">
    <br>

    <!-- personal image -->
    <div class="profile-img col-md-4">
      <div class="team-member">
        <a href="https://ayameyao.github.io/">
          <img class="mx-auto rounded-circle" src="{{site.baseurl}}/static/img/profile/guangyaoli.jpg" />
          <h4>Guangyao Li</h4></a>
          <h5>PhD Candidate</h5>
          <h5>(Sep 2020 - )</h5>
          <h6 class="text-muted">Renmin University of China</h6>
      </div>
    </div>


    <div class="profile-img col-md-8" style="float:right">
      <h1>Biography</h1>
      <p>
        My name is Li Guangyao! and I am a second-year PhD Candidate in the Gaoling School of Artificial Intelligence (GSAI), Renmin University of China.
      </p>

      <div class="row">
        <div class="col-lg-12 text-left">
          <h4 class="section-subheading" style="text-align:left;">Research area</h4>
          <ul>
            <li>MultiModal Machine Learning</li>
            <li>MultiModal Scene Understanding</li>
          </ul>
        </div>
      </div>
    </div>

  </div>
</section>




<!-- Publications -->
<section class="bg-light" id="publications" style="clear:both;">
  <div class="container">
    <div class="row justify-content-md-center text-center">
      <div class="col-md-12" style="text-align:left">
        <h4 class="section-subheading" style="text-align:left; margin-left:-14px">Publications</h4>
      </div>

      <hr/>
      <p class="text-muted" style="text-align:left">
        Some video examples with QA pairs in the MUSIC-AVQA dataset. 
        Through these examples, we can have a better understanding of the dataset, and can more intuitively feel the QA tasks in dynamic and complex audio-visual scenes
      </p>

      <!-- publication 01 -->
      <div style="display: grid;grid-template-columns: 2% 28% 68% 2%;">
        <div>
          <img src="{{ site.baseurl }}/static/files/publications/music-avqa.png" style="width:100%;height:100%;box-shadow: -5px 5px 20px #eee;border-radius:3px;" />
        </div>
        <div style="margin-left: 16px;">
          <div style="background-color: #fff;padding: 20px !important;text-align:left; box-shadow: -5px 5px 20px #eee;border-radius:3px;" class="text-muted">
            <h5>Learning to Answer Questions in Dynamic Audio-Visual Scenarios</h5>
            <p>
              Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji-Rong Wen and Di Hu
            </p>
            <p>
              Proc. IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2022.
            </p>
            <p class="text-muted" style="color:#00F">
              <a href="{{ site.baseurl }}/static/files/MUSIC-AVQA.pdf" style="color:#ffc107">[Paper]&nbsp;
              </a><a href="{{ site.baseurl }}/static/files/MUSIC-AVQA-supp.pdf" style="color:#ffc107">[Supplementary]</a>&nbsp;
              <a href="https://arxiv.org/abs/2203.14072" style="color:#ffc107">[arXiv]</a>&nbsp;
              <a href="{{ site.baseurl }}/static/files/MUSIC-AVQA-poster.pdf" style="color:#ffc107">[Poster]</a>&nbsp;
              <!-- <a href="https://www.youtube.com/watch?v=JH3t5gwe9Xw" style="color:#ffc107">[YouTube]</a>&nbsp; -->
              <a href="https://www.bilibili.com/video/BV1Br4y1q7YN/" style="color:#ffc107">[Bilibili]</a>&nbsp;
              <a href="https://github.com/GeWu-Lab/MUSIC-AVQA" style="color:#ffc107">[Code]</a>
            </p>
          </div>
        </div>
      </div>

      <br>

      <!-- publication 02 -->
      <div style="display: grid;grid-template-columns: 2% 28% 68% 2% ;">
        <div>
          <img src="{{ site.baseurl }}/static/files/publications/LooktoHearOurPlanet.png" style="width:100%;height:100%;box-shadow: -5px 5px 20px #eee;border-radius:3px;" />
        </div>
        <div style="margin-left: 16px;">
          <div style="background-color: #fff;padding: 20px !important;text-align:left; box-shadow: -5px 5px 20px #eee;border-radius:3px;" class="text-muted">
            <h5>Learning to Answer Questions in Dynamic Audio-Visual Scenarios</h5>
            <p>
              Konrad Heidler, Lichao Mou, Di Hu*, Pu Jin, <b>Guangyao Li</b>, Chuang Gan, Ji-Rong Wen, Xiao Xiang Zhu
            </p>
            <p>
              arXiv preprint arXiv:2108.00688.
            </p>
            <p class="text-muted" style="color:#00F">
              <a href="https://arxiv.org/abs/2108.00688" style="color:#ffc107">[arXiv]&nbsp;
              <a href="hhttps://www.youtube.com/watch?v=gD_fNJPBWhs" style="color:#ffc107">[Demo]</a>&nbsp;
            </p>
          </div>
        </div>
      </div>




    </div>
    </div>
  </div>
</section>


<!-- Contact -->
<section id="contact">
    <div class="container">
      <div class="row">


        <div class="col-lg-12">
          <h2 class="section-heading text-uppercase">Contact</h2>
          <div class="text-muted">
              Email: guangyaoli@ruc.edu.cn
          </div>
        </div>

      </div>
    </div>
</section>



<script type="application/ld+json">
{
  "@context":"http://schema.org/",
  "@type":"Dataset",
  "name":"MUSIC-AVQA dataset",
  "description":"First-person (egocentric) video dataset; multi-faceted non-scripted recordings in the wearers' homes, capturing all daily activities in the kitchen over multiple days. Annotations are collected using a novel live audio commentary approach.",
  "url":"https://github.com/epic-kitchens/annotations",
  "sameAs":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d",
  "citation":"Damen, Dima et al. 'Scaling Egocentric Vision: The EPIC-KITCHENS Dataset', European Conference on Computer Vision, 2018",
  "identifier": "10.5523/bris.3h91syskeag572hl6tvuovwv4d",
  "keywords":[
     "Egocentric vision",
     "Human actions",
     "Object interactions",
     "actions",
     "video",
     "kitchens",
     "cooking",
     "dataset",
     "epic kitchens",
     "epic",
     "eccv",
     "2022"
  ],
  "creator":{
     "@type":"Organization",
     "url": "https://epic-kitchens.github.io/",
     "name":"EPIC Team",
     "contactPoint":{
        "@type":"ContactPoint",
        "contactType": "technical support",
        "email":"uob-epic-kitchens@bristol.ac.uk",
        "url":"https://github.com/epic-kitchens/annotations/issues"
     }
  },
  "distribution":[
     {
        "@type":"DataDownload",
        "encodingFormat":"video/mp4",
        "contentUrl":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"image/jpeg",
        "contentUrl":"https://data.bris.ac.uk/data/dataset/3h91syskeag572hl6tvuovwv4d"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"text/csv",
        "contentUrl":"https://github.com/epic-kitchens/annotations"
     },
     {
        "@type":"DataDownload",
        "encodingFormat":"application/octet-stream",
        "contentUrl":"https://github.com/epic-kitchens/annotations"
     }
  ],
  "license": "https://creativecommons.org/licenses/by-nc/4.0/"
}
</script>

